{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6lkbVdeTRkm",
        "outputId": "41ca664b-4b58-4973-db71-09e11de0a6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.10/dist-packages (0.98)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate wget bz2file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlIUwuOMS_mb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AlbertTokenizer, AlbertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, load\n",
        "import random\n",
        "import numpy as np\n",
        "import bz2\n",
        "import wget\n",
        "import requests\n",
        "import shutil\n",
        "from math import exp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQBbjFxFS_md"
      },
      "source": [
        "### Downloading the original datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yjTsUqHS_me"
      },
      "outputs": [],
      "source": [
        "# Download BOOKCORPUS\n",
        "print(\"Downloading BOOKCORPUS...\")\n",
        "bookcorpus_dataset = load_dataset(\"bookcorpus\", split=\"train\")\n",
        "bookcorpus_data = bookcorpus_dataset[\"train\"]\n",
        "bookcorpus_text_file = \"./train_data_bookcorpus.txt\"\n",
        "\n",
        "with open(bookcorpus_text_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for example in tqdm(bookcorpus_data, desc=\"Writing to file\", unit=\"example\"):\n",
        "        f.write(example[\"text\"] + \"\\n\")\n",
        "\n",
        "print(\"BOOKCORPUS saved to\", bookcorpus_text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVknR894S_mf"
      },
      "outputs": [],
      "source": [
        "def download_file(url, output_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get(\"content-length\", 0))\n",
        "    block_size = 1024  # 1 Kilobyte\n",
        "    t = tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=\"Downloading Wikipedia dump\")\n",
        "\n",
        "    with open(output_path, \"wb\") as file:\n",
        "        for data in response.iter_content(block_size):\n",
        "            t.update(len(data))\n",
        "            file.write(data)\n",
        "    t.close()\n",
        "\n",
        "    if total_size != 0 and t.n != total_size:\n",
        "        print(\"ERROR: Something went wrong during the download\")\n",
        "    else:\n",
        "        print(\"\\nWikipedia dump downloaded successfully.\")\n",
        "\n",
        "\n",
        "# Download Wikipedia dump\n",
        "print(\"Downloading Wikipedia dump...\")\n",
        "dump_url = \"https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\"\n",
        "dump_file = \"./enwiki-latest-pages-articles.xml.bz2\"\n",
        "\n",
        "if not os.path.exists(dump_file):\n",
        "    download_file(dump_url, dump_file)\n",
        "else:\n",
        "    print(\"Wikipedia dump already exists.\")\n",
        "\n",
        "# Extract Wikipedia dump with resume capability\n",
        "print(\"Extracting Wikipedia dump...\")\n",
        "xml_file = \"./enwiki-latest-pages-articles.xml\"\n",
        "try:\n",
        "    total_size = os.path.getsize(dump_file)  # Size of the compressed file\n",
        "    extracted_size = os.path.getsize(xml_file) if os.path.exists(xml_file) else 0\n",
        "\n",
        "    with bz2.BZ2File(dump_file, \"rb\") as infile, open(xml_file, \"ab\") as outfile:\n",
        "        # Skip already extracted portion with progress tracking\n",
        "        processed_bytes = 0\n",
        "        with tqdm(total=extracted_size, unit=\"iB\", unit_scale=True, desc=\"Skipping extracted portion\") as skip_tqdm:\n",
        "            while processed_bytes < extracted_size:\n",
        "                read_size = min(1024, extracted_size - processed_bytes)\n",
        "                infile.read(read_size)  # Read but don't write, to skip already processed\n",
        "                processed_bytes += read_size\n",
        "                skip_tqdm.update(read_size)\n",
        "\n",
        "        # Continue extraction with progress tracking\n",
        "        with tqdm(total=total_size, initial=infile.tell(), unit=\"iB\", unit_scale=True, desc=\"Extracting Wikipedia dump\") as t:\n",
        "            while True:\n",
        "                data = infile.read(1024)\n",
        "                if not data:\n",
        "                    break\n",
        "                outfile.write(data)\n",
        "                t.update(len(data))\n",
        "\n",
        "    print(\"Extraction completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR: Extraction failed. Please check the input file.\")\n",
        "    print(e)\n",
        "\n",
        "# Combine all extracted text files into one\n",
        "wikipedia_text_file = \"./train_data_wikipedia.txt\"\n",
        "max_size = 4.5 * 1024 * 1024 * 1024\n",
        "current_size = 0\n",
        "\n",
        "if os.path.exists(xml_file):\n",
        "    with open(xml_file, \"r\", encoding=\"utf-8\") as infile, open(wikipedia_text_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        with tqdm(total=max_size, unit=\"iB\", unit_scale=True, desc=\"Writing extracted XML to file (limited to 4.5 GB)\") as t:\n",
        "            for line in infile:\n",
        "                line_size = len(line.encode(\"utf-8\"))\n",
        "                if current_size + line_size > max_size:\n",
        "                    break\n",
        "                outfile.write(line)\n",
        "                current_size += line_size\n",
        "                t.update(line_size)\n",
        "        print(\"Wikipedia text saved to\", wikipedia_text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWhCEJezS_mf"
      },
      "outputs": [],
      "source": [
        "# Combine the two text files into one\n",
        "combined_text_file = \"./combined_train_data.txt\"\n",
        "text_files = [\"./train_data_bookcorpus.txt\", \"./train_data_wikipedia.txt\"]\n",
        "if not os.path.exists(combined_text_file):\n",
        "    with open(combined_text_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        for text_file in text_files:\n",
        "            with open(text_file, \"r\", encoding=\"utf-8\") as infile:\n",
        "                for line in tqdm(infile, desc=f\"Combining {text_file}\"):\n",
        "                    outfile.write(line)\n",
        "    print(f\"Combined text saved to {combined_text_file}\")\n",
        "else:\n",
        "    print(\"Combined text file already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUbm6QL1S_mf"
      },
      "source": [
        "### Dataset and Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QxXPaDiTCce",
        "outputId": "7a6f713d-b649-4bed-e17a-bb42dd7364b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvxC9yi7S_mg"
      },
      "outputs": [],
      "source": [
        "# Define Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length):\n",
        "        self.file_path = file_path\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # Read all lines into memory for faster access\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            self.lines = file.readlines()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line = self.lines[idx].strip()\n",
        "        inputs = self.tokenizer(\n",
        "            line,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {key: val.squeeze(0) for key, val in inputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cbs6KZiS_mg",
        "outputId": "b83d2fb3-7744-4058-fed7-85000bd72d59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztWytbsBS_mg",
        "outputId": "82c7460d-e55d-457f-d195-e9fa9e09f7b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Define constants\n",
        "COMBINED_TEXT_FILE = \"/content/drive/MyDrive/UTS/Advanced NLP/subset_train_data_xSmall.txt\"\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 3\n",
        "MAX_SEQ_LENGTH = 256\n",
        "LEARNING_RATE = 1e-5\n",
        "SUBSET_FRAC = 0.00005\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj8UlmkES_mh"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = TextDataset(COMBINED_TEXT_FILE, tokenizer, MAX_SEQ_LENGTH)\n",
        "\n",
        "# Take a fraction of the dataset\n",
        "subset_size = int(SUBSET_FRAC * len(dataset))\n",
        "subset_indices = range(subset_size)\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "\n",
        "# Save the subset of the dataset to a new .txt file\n",
        "subset_file = \"./subset_train_data.txt\"\n",
        "with open(subset_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in tqdm(subset_indices, desc=\"Saving subset\", unit=\"line\"):\n",
        "        # Decode the tokenized input back to text\n",
        "        original_text = dataset.tokenizer.decode(dataset[i]['input_ids'], skip_special_tokens=True)\n",
        "        f.write(original_text + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JS_AC1sS_mh",
        "outputId": "3a6bfa78-a139-44a8-dc38-440d7ad480cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "def load_data(tokenizer, batch_size):\n",
        "    dataset = TextDataset(COMBINED_TEXT_FILE, tokenizer, MAX_SEQ_LENGTH)\n",
        "\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_dataset, val_dataset, train_dataloader, val_dataloader\n",
        "\n",
        "\n",
        "# Create dataset and data loader\n",
        "print(\"Loading dataset...\")\n",
        "train_dataset, val_dataset, train_dataloader, val_dataloader = load_data(tokenizer, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJfowE14S_mh"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpvgGHkDS_mh",
        "outputId": "169e15f7-548e-4a4e-a2ed-c84f04b56ab9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained ALBERT model\n",
        "model = AlbertForMaskedLM.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "# # Freeze all layers\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # Unfreeze the last layer group\n",
        "# for param in model.albert.encoder.albert_layer_groups[-1].parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# # Move model to device\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWbDc62XS_mh",
        "outputId": "0151f1c1-867b-45c2-df3b-d0231cffada5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/UTS/Advanced NLP/results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    save_total_limit=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rna8EqqcS_mi",
        "outputId": "05ac0250-f140-48da-b9a4-cfde132886d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing data collator...\n"
          ]
        }
      ],
      "source": [
        "# Data collator for masked language modeling\n",
        "print(\"Preparing data collator...\")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMRUxmNmS_mi"
      },
      "outputs": [],
      "source": [
        "# Load accuracy metric\n",
        "from evaluate import load # Import load instead of load_metric\n",
        "accuracy_metric = load(\"accuracy\") # Use load instead of load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Flatten the predictions and labels\n",
        "    preds_flat = predictions.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    # Create a mask to filter out padding tokens (-100)\n",
        "    mask = labels_flat != -100\n",
        "    # Apply the mask\n",
        "    preds_flat = preds_flat[mask]\n",
        "    labels_flat = labels_flat[mask]\n",
        "    # Avoid calculations if there are no valid labels\n",
        "    if len(labels_flat) == 0:\n",
        "        return {\"accuracy\": 0}\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_metric.compute(predictions=preds_flat, references=labels_flat)[\"accuracy\"]\n",
        "    return {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBaDJR87S_mi",
        "outputId": "8f05502c-5061-412e-a534-a5d24e8b7793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Trainer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Initialize Trainer\n",
        "print(\"Initializing Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gJttP_ZCS_mi",
        "outputId": "01e2133a-d79b-42a1-f73d-3b2af0c2fd94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='783' max='783' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [783/783 01:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.644351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.053000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.653846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.053000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.677130</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=783, training_loss=1.8808303384793064, metrics={'train_runtime': 113.1583, 'train_samples_per_second': 27.652, 'train_steps_per_second': 6.92, 'total_flos': 35161211215872.0, 'train_loss': 1.8808303384793064, 'epoch': 3.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "KFnICAh7S_mi",
        "outputId": "0abdbf0f-04c6-4ded-a711-c6ca049f977b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [116/116 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': nan, 'eval_accuracy': 0.7061611374407583, 'eval_runtime': 7.1992, 'eval_samples_per_second': 16.113, 'eval_steps_per_second': 16.113, 'epoch': 3.0}\n",
            "Test Accuracy: 0.7062\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "print(\"Evaluating model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "accuracy = eval_results.get('eval_accuracy')\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF5fnpJlS_mi",
        "outputId": "8d2f8d57-33f4-4c70-ce6e-e1c154506165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the model...\n",
            "Training complete and model saved!\n"
          ]
        }
      ],
      "source": [
        "# Save the final model\n",
        "print(\"Saving the model...\")\n",
        "model.save_pretrained('/content/drive/MyDrive/UTS/Advanced NLP/trained_albert')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/UTS/Advanced NLP/trained_albert')\n",
        "print(\"Training complete and model saved!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
